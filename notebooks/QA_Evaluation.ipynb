{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "LOCAL_PATH  = \"local_run.json\"\n",
        "GLOBAL_PATH = \"global_run.json\"\n",
        "FULL_PATH   = \"full_run.json\"\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1. load both runs into dict[query] → record\n",
        "# --------------------------------------------------------------------\n",
        "with open(LOCAL_PATH,  encoding=\"utf-8\") as f:\n",
        "    local_run  = {r[\"query\"]: r for r in json.load(f)}\n",
        "\n",
        "with open(GLOBAL_PATH, encoding=\"utf-8\") as f:\n",
        "    global_run = {r[\"query\"]: r for r in json.load(f)}\n",
        "\n",
        "assert local_run.keys() == global_run.keys(), \"Local & global sets differ!\"\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. merge according to question_type\n",
        "# --------------------------------------------------------------------\n",
        "full_run = []\n",
        "for q, local_rec in local_run.items():\n",
        "    rec = (local_rec if local_rec[\"question_type\"] == \"inference_query\"\n",
        "           else global_run[q])\n",
        "    full_run.append(rec)\n",
        "\n",
        "print(f\"✓ merged {len(full_run)} records → {FULL_PATH}\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. save\n",
        "# --------------------------------------------------------------------\n",
        "with open(FULL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(full_run, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "KHGSnwKigcFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c20d9f-402d-4a23-86e4-a4b9b6d36ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ merged 2556 records → full_run.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4hkNtzf2BWJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(fname, q_file='MultiHopRAG.json'):\n",
        "  # Read files\n",
        "  with open(fname, 'r') as file:\n",
        "      doc_data = json.load(file)\n",
        "\n",
        "  with open(q_file, 'r') as file:\n",
        "      query_data = json.load(file)\n",
        "\n",
        "  # Initialize dictionary to save lists of predictions and gold standards for each question_type\n",
        "  type_data = {}\n",
        "  overall_pred_list = []\n",
        "  overall_gold_list = []\n",
        "\n",
        "  # Function to get the correct answer\n",
        "  def get_gold(query):\n",
        "      for q in query_data:\n",
        "          if q['query'] == query:\n",
        "              return q['answer']\n",
        "      return ''\n",
        "\n",
        "  # Function to check if there is an intersection of words between two strings\n",
        "  def has_intersection(a, b):\n",
        "      a_words = set(a.split())\n",
        "      b_words = set(b.split())\n",
        "      return len(a_words.intersection(b_words)) > 0\n",
        "\n",
        "  # Function to extract the answer\n",
        "  def extract_answer(input_string):\n",
        "      match = re.search(r'The answer to the question is \"(.*?)\"', input_string)\n",
        "      return match.group(1) if match else input_string\n",
        "\n",
        "  # Main loop, iterate through document data\n",
        "  for d in tqdm(doc_data):\n",
        "      model_answer = d['model_answer']\n",
        "      if 'The answer' in model_answer:\n",
        "          model_answer = extract_answer(model_answer)\n",
        "      gold = get_gold(d['query'])\n",
        "      if gold:\n",
        "          question_type = d['question_type']\n",
        "          if question_type not in type_data:\n",
        "              type_data[question_type] = {'pred_list': [], 'gold_list': []}\n",
        "          type_data[question_type]['pred_list'].append(model_answer)\n",
        "          type_data[question_type]['gold_list'].append(gold)\n",
        "          overall_pred_list.append(model_answer)\n",
        "          overall_gold_list.append(gold)\n",
        "\n",
        "  # Function to calculate evaluation metrics\n",
        "  def calculate_metrics(pred_list, gold_list):\n",
        "      tp = sum(1 for pred, gold in zip(pred_list, gold_list) if has_intersection(pred.lower(), gold.lower()))\n",
        "      fp = sum(1 for pred, gold in zip(pred_list, gold_list) if not has_intersection(pred.lower(), gold.lower()))\n",
        "      fn = len(gold_list) - tp\n",
        "      tn = len(pred_list) - tp\n",
        "\n",
        "      precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "      recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "      f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "      accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "\n",
        "      return precision, recall, f1, accuracy\n",
        "\n",
        "  # Output evaluation data for each question_type\n",
        "  for question_type, data in type_data.items():\n",
        "      precision, recall, f1, accuracy = calculate_metrics(data['pred_list'], data['gold_list'])\n",
        "      print(f\"Question Type: {question_type}\")\n",
        "      print(f\" Precision: {precision:.3f}\")\n",
        "      print(f\" Recall: {recall:.3f}\")\n",
        "      print(f\" F1 Score: {f1:.3f}\")\n",
        "      print(f\" accuracy: {accuracy:.3f}\")\n",
        "\n",
        "  # Calculate overall evaluation metrics\n",
        "  overall_precision, overall_recall, overall_f1, overall_accuracy = calculate_metrics(overall_pred_list, overall_gold_list)\n",
        "  print(f\"Overall Metrics:\")\n",
        "  print(f\" Precision: {overall_precision:.3f}\")\n",
        "  print(f\" Recall: {overall_recall:.3f}\")\n",
        "  print(f\" F1 Score: {overall_f1:.3f}\")\n",
        "  print(f\" Accuracy: {overall_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "Cmk11glSBIqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval('full_run_len_600_limit_50.json')\n",
        "# eval('deep_full_run_len_600_limit_50.json')\n",
        "eval('full_run.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb5Pg-6qBhxT",
        "outputId": "bdcf9104-ac74-4a4d-df42-e8b35392efca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2556/2556 [00:00<00:00, 14395.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.920\n",
            " Recall: 0.920\n",
            " F1 Score: 0.920\n",
            " accuracy: 0.863\n",
            "Question Type: comparison_query\n",
            " Precision: 0.210\n",
            " Recall: 0.210\n",
            " F1 Score: 0.210\n",
            " accuracy: 0.388\n",
            "Question Type: null_query\n",
            " Precision: 0.246\n",
            " Recall: 0.246\n",
            " F1 Score: 0.246\n",
            " accuracy: 0.399\n",
            "Question Type: temporal_query\n",
            " Precision: 0.415\n",
            " Recall: 0.415\n",
            " F1 Score: 0.415\n",
            " accuracy: 0.461\n",
            "Overall Metrics:\n",
            " Precision: 0.488\n",
            " Recall: 0.488\n",
            " F1 Score: 0.488\n",
            " Accuracy: 0.494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval('local_run_len_600_limit_50.json')\n",
        "# eval('local_run_limit_10.json')\n",
        "eval('local_run.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFHsh0I8BpEu",
        "outputId": "026e9a08-dffc-4b19-88a7-f41b6365d090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2556/2556 [00:00<00:00, 15459.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.920\n",
            " Recall: 0.920\n",
            " F1 Score: 0.920\n",
            " accuracy: 0.863\n",
            "Question Type: comparison_query\n",
            " Precision: 0.072\n",
            " Recall: 0.072\n",
            " F1 Score: 0.072\n",
            " accuracy: 0.350\n",
            "Question Type: null_query\n",
            " Precision: 0.033\n",
            " Recall: 0.033\n",
            " F1 Score: 0.033\n",
            " accuracy: 0.341\n",
            "Question Type: temporal_query\n",
            " Precision: 0.148\n",
            " Recall: 0.148\n",
            " F1 Score: 0.148\n",
            " accuracy: 0.370\n",
            "Overall Metrics:\n",
            " Precision: 0.356\n",
            " Recall: 0.356\n",
            " F1 Score: 0.356\n",
            " Accuracy: 0.437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval('global_run_len_600_limit_50.json')\n",
        "# eval('deep_global_run_len_600_limit_50.json')\n",
        "eval('global_run.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ALPf4G6B8T1",
        "outputId": "db3d1858-d00d-487a-f277-06403869c269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2556/2556 [00:00<00:00, 15251.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.450\n",
            " Recall: 0.450\n",
            " F1 Score: 0.450\n",
            " accuracy: 0.476\n",
            "Question Type: comparison_query\n",
            " Precision: 0.210\n",
            " Recall: 0.210\n",
            " F1 Score: 0.210\n",
            " accuracy: 0.388\n",
            "Question Type: null_query\n",
            " Precision: 0.246\n",
            " Recall: 0.246\n",
            " F1 Score: 0.246\n",
            " accuracy: 0.399\n",
            "Question Type: temporal_query\n",
            " Precision: 0.415\n",
            " Recall: 0.415\n",
            " F1 Score: 0.415\n",
            " accuracy: 0.461\n",
            "Overall Metrics:\n",
            " Precision: 0.338\n",
            " Recall: 0.338\n",
            " F1 Score: 0.338\n",
            " Accuracy: 0.430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in ['qwen_llm_run.json', 'qwen_rag_run.json']:\n",
        "    print(fname)\n",
        "    eval(fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nFTXawJB_X9",
        "outputId": "2b43ba40-78a6-479a-b1a6-9e72a2c3e641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen_llm_run.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2556/2556 [00:00<00:00, 15502.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.645\n",
            " Recall: 0.645\n",
            " F1 Score: 0.645\n",
            " accuracy: 0.585\n",
            "Question Type: comparison_query\n",
            " Precision: 0.030\n",
            " Recall: 0.030\n",
            " F1 Score: 0.030\n",
            " accuracy: 0.340\n",
            "Question Type: null_query\n",
            " Precision: 0.043\n",
            " Recall: 0.043\n",
            " F1 Score: 0.043\n",
            " accuracy: 0.343\n",
            "Question Type: temporal_query\n",
            " Precision: 0.106\n",
            " Recall: 0.106\n",
            " F1 Score: 0.106\n",
            " accuracy: 0.359\n",
            "Overall Metrics:\n",
            " Precision: 0.245\n",
            " Recall: 0.245\n",
            " F1 Score: 0.245\n",
            " Accuracy: 0.399\n",
            "qwen_rag_run.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2556/2556 [00:00<00:00, 15914.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.886\n",
            " Recall: 0.886\n",
            " F1 Score: 0.886\n",
            " accuracy: 0.814\n",
            "Question Type: comparison_query\n",
            " Precision: 0.090\n",
            " Recall: 0.090\n",
            " F1 Score: 0.090\n",
            " accuracy: 0.355\n",
            "Question Type: null_query\n",
            " Precision: 0.233\n",
            " Recall: 0.233\n",
            " F1 Score: 0.233\n",
            " accuracy: 0.394\n",
            "Question Type: temporal_query\n",
            " Precision: 0.170\n",
            " Recall: 0.170\n",
            " F1 Score: 0.170\n",
            " accuracy: 0.376\n",
            "Overall Metrics:\n",
            " Precision: 0.379\n",
            " Recall: 0.379\n",
            " F1 Score: 0.379\n",
            " Accuracy: 0.446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Sequence\n",
        "\n",
        "def filter_json_by_indices(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    indices_path: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Keep only the elements whose 0-based positions are listed in *indices_path*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_path   : file with the full list of dicts (e.g. answers_all.json)\n",
        "    output_path  : where the 125-item subset should be written\n",
        "    indices_path : JSON file containing a list of integers, already sorted\n",
        "                   (the same indices you saved when sampling the questions)\n",
        "    \"\"\"\n",
        "    # load everything\n",
        "    with open(input_path, encoding=\"utf-8\") as f:\n",
        "        full_list = json.load(f)\n",
        "\n",
        "    with open(indices_path, encoding=\"utf-8\") as f:\n",
        "        keep = json.load(f)\n",
        "\n",
        "    # light sanity check\n",
        "    if keep and (keep[-1] >= len(full_list) or min(keep) < 0):\n",
        "        raise IndexError(\"Index list contains out-of-range values.\")\n",
        "\n",
        "    # slice – order is preserved because *keep* is sorted\n",
        "    filtered = [full_list[i] for i in keep]\n",
        "\n",
        "    # save\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(filtered, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✓ wrote {len(filtered)} items → {output_path}\")\n",
        "\n",
        "\n",
        "# --- quick example --------------------------------------------------\n",
        "# filter_json_by_indices(\n",
        "#     input_path=\"answers_all.json\",\n",
        "#     output_path=\"answers_sampled.json\",\n",
        "#     indices_path=\"sampled_indices.json\",\n",
        "# )\n"
      ],
      "metadata": {
        "id": "PPEDxxBwTKgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in ['qwen_rag_run.json', 'full_run.json']:\n",
        "    filter_json_by_indices(\n",
        "        input_path=fname,\n",
        "        output_path='sample_'+fname,\n",
        "        indices_path=\"sampled_indices.json\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6jN3zvkTP6i",
        "outputId": "391033e9-6933-4481-c2e4-0bf2b9aea4de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ wrote 125 items → sample_qwen_rag_run.json\n",
            "✓ wrote 125 items → sample_full_run.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in ['qwen_rag_run.json', 'full_run.json']:\n",
        "    print(fname)\n",
        "    eval('sample_'+fname, 'sampled_queries.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyoNPHwqTqfI",
        "outputId": "163e640a-5c0f-4084-c8a7-7dee3392f37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qwen_rag_run.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 159019.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 1.000\n",
            " Recall: 1.000\n",
            " F1 Score: 1.000\n",
            " accuracy: 1.000\n",
            "Question Type: comparison_query\n",
            " Precision: 0.167\n",
            " Recall: 0.167\n",
            " F1 Score: 0.167\n",
            " accuracy: 0.375\n",
            "Question Type: temporal_query\n",
            " Precision: 0.143\n",
            " Recall: 0.143\n",
            " F1 Score: 0.143\n",
            " accuracy: 0.368\n",
            "Question Type: null_query\n",
            " Precision: 0.400\n",
            " Recall: 0.400\n",
            " F1 Score: 0.400\n",
            " accuracy: 0.455\n",
            "Overall Metrics:\n",
            " Precision: 0.456\n",
            " Recall: 0.456\n",
            " F1 Score: 0.456\n",
            " Accuracy: 0.479\n",
            "full_run.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 156550.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.975\n",
            " Recall: 0.975\n",
            " F1 Score: 0.975\n",
            " accuracy: 0.952\n",
            "Question Type: comparison_query\n",
            " Precision: 0.190\n",
            " Recall: 0.190\n",
            " F1 Score: 0.190\n",
            " accuracy: 0.382\n",
            "Question Type: temporal_query\n",
            " Precision: 0.321\n",
            " Recall: 0.321\n",
            " F1 Score: 0.321\n",
            " accuracy: 0.424\n",
            "Question Type: null_query\n",
            " Precision: 0.267\n",
            " Recall: 0.267\n",
            " F1 Score: 0.267\n",
            " accuracy: 0.405\n",
            "Overall Metrics:\n",
            " Precision: 0.480\n",
            " Recall: 0.480\n",
            " F1 Score: 0.480\n",
            " Accuracy: 0.490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fname in ['ms_local.json', 'ms_global.json']:\n",
        "    print(fname)\n",
        "    eval(fname, 'sampled_queries.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2CljbroT7BK",
        "outputId": "6e1f0f75-0b79-4a96-fb07-a501354b67d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ms_local.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 145031.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.975\n",
            " Recall: 0.975\n",
            " F1 Score: 0.975\n",
            " accuracy: 0.952\n",
            "Question Type: comparison_query\n",
            " Precision: 0.071\n",
            " Recall: 0.071\n",
            " F1 Score: 0.071\n",
            " accuracy: 0.350\n",
            "Question Type: temporal_query\n",
            " Precision: 0.107\n",
            " Recall: 0.107\n",
            " F1 Score: 0.107\n",
            " accuracy: 0.359\n",
            "Question Type: null_query\n",
            " Precision: 0.000\n",
            " Recall: 0.000\n",
            " F1 Score: 0.000\n",
            " accuracy: 0.333\n",
            "Overall Metrics:\n",
            " Precision: 0.360\n",
            " Recall: 0.360\n",
            " F1 Score: 0.360\n",
            " Accuracy: 0.439\n",
            "ms_global.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:00<00:00, 29188.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question Type: inference_query\n",
            " Precision: 0.750\n",
            " Recall: 0.750\n",
            " F1 Score: 0.750\n",
            " accuracy: 0.667\n",
            "Question Type: comparison_query\n",
            " Precision: 0.071\n",
            " Recall: 0.071\n",
            " F1 Score: 0.071\n",
            " accuracy: 0.350\n",
            "Question Type: temporal_query\n",
            " Precision: 0.179\n",
            " Recall: 0.179\n",
            " F1 Score: 0.179\n",
            " accuracy: 0.378\n",
            "Question Type: null_query\n",
            " Precision: 0.067\n",
            " Recall: 0.067\n",
            " F1 Score: 0.067\n",
            " accuracy: 0.349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Metrics:\n",
            " Precision: 0.312\n",
            " Recall: 0.312\n",
            " F1 Score: 0.312\n",
            " Accuracy: 0.421\n"
          ]
        }
      ]
    }
  ]
}